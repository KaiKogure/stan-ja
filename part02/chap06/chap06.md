## 6. 回帰モデル

Stanでは、単純な線形回帰からマルチレベルの一般化線形モデルまでの回帰モデルが使えます。

### 6.1. 線形回帰

以下は最も単純な線形回帰モデルで、1つの予測変数と、傾きと切片の係数があり、ノイズは正規分布です。

![$$ y_{n} = \alpha + \beta x_{n} + \epsilon_{n} \quad\text{ここで}\quad \epsilon_{n} \sim \mathsf{Normal}(0, \sigma) $$](fig/fig01.png)

これは、以下のように残差を含めてサンプリングするのと等価です。

![$$ y_{n} - (\alpha + \beta X_{n}) \sim \mathsf{Normal}(0, \sigma) $$](fig/fig02.png)

さらに短くなります。

![$$ y_{n} \sim \mathsf{Normal}(\alpha + \beta X_{n}, \sigma) $$](fig/fig03.png)

このモデルの最後の形はStanでは以下のようにコーディングします。

```
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}
```

`N`回の観測があり、それぞれに予測値`x[n]`と結果`y[n]`があります。切片と傾きのパラメータは`alpha`と`beta`です。このモデルでは、スケール`sigma`の、正規分布するノイズ項を仮定しています。また、2つの回帰係数には非正則事前分布が設定されています。

#### 行列記法とベクトル化

前のモデルのサンプリング文はベクトル化されています。

```
y ~ normal(alpha + beta * x, sigma);
```

同じモデルの、ベクトル化されていないバージョンは以下のとおりです。

```
for (n in 1:N)
  y[n] ~ normal(alpha + beta * x[n], sigma);
```

より簡潔なことに加えて、ベクトル化された形の方がはるかに高速です。<sup>1</sup>

一般にStanでは、`normal`のような分布に渡す引数はベクトルにすることができます。他の引数のいずれかがベクトルまたは配列なら、同じサイズでなくてはなりません。多の引数のいずれかがスカラーなら、ベクトルの各要素に再利用されます。確率関数のベクトル化についてのより詳しい情報は37.5節を参照してください。

この書き方がうまくいく他の理由は、行列には行列演算を行なうように、Stanの算術演算子がオーバーロードされるからです。この場合では、`x`が`vector`型で`beta`が`real`型なので、式`beta * x`は`vector`型です。Stanはベクトル化をサポートしているので、2つ以上の予測変数がある回帰モデルもそのまま行列の記法を用いて書くことができます。

```
data {
  int<lower=0> N;  // number of data items
  int<lower=0> K;  // number of predictors
  matrix[N,K] x;   // predictor matrix
  vector[N] y;     // outcome vector
}
parameters {
  real alpha;           // intercept
  vector[K] beta;       // coefficients for predictors
  real<lower=0> sigma;  // error scale
}
model {
  y ~ normal(x * beta + alpha, sigma);  // likelihood
}
```

`sigma`の宣言には`lower=0`という制約をつけて、値が0以上になるように制限しています。`model`ブロックには事前分布がないので、非負の実数の非正則事前分布ということになります。より情報のある事前分布を加えることもできますが、正則事後分布が導ける限り、非正則事前分布も使えます。

上のモデルでは、`x`は$N \times K$行列の予測変数、`beta`は$K$次元ベクトルの係数なので、`x * beta`は$N$次元ベクトルの予測値です。$N$個のデータ項目のそれぞれに対応します。これら予測値は、$N$次元ベクトル`y`にある結果に対応して揃っていますので、上のようにモデル全体を行列演算を使って書くことができます。値が1の列を`x`に含めることにより、`alpha`パラメータをなくすこともできるでしょう。

上のモデルのサンプリング文は、統計的に等価な以下のモデルのような、ループを使ったモデルを、より効率的に、ベクトルによる方法でコーディングしただけです。
```
model {
  for (n in 1:N)
    y[n] ~ normal(x[n] * beta, sigma);
}
```

Stanの行列のインデキシング方式では、`x[n]`は行列`x`の行`n`を取り出します。`beta`は列ベクトルなので、積`x[n] * beta`は`real`型のスカラーです。

##### 入力に含めた切片

以下のモデル定式化では、切片の係数`alpha`はもうありません。

```
y ~ normal(x * beta, sigma);
```

そのかわり、入力行列`x`の最初の列が、値が1の列であると仮定しています。この方法では、`beta[1]`が切片の役割を果たしているのです。もし切片が、傾きの項とは異なる事前分布を取っているなら、違いがはっきりするでしょう（**be clearer to break it out: 訳はこれでよい?**）。乗算の回数が1つ減るので、係数の変数を明示的に別にする形式よりもやや効率的でもあります。といっても、速度にはたいした違いはでないでしょうから、これを選ぶ理由は明確さにあるでしょう。

<sup>1</sup> PythonやRはインタプリタとして動作しますが、それとは異なりStanはC++に変換されてコンパイルされます。そのため、ループと代入文は高速です。ベクトル化されたコードがStanで高速なのは以下の理由によります。(a) 導関数を計算するのに使われる式木が単純にできます。これにより、仮想関数の呼び出しが少なくなります。(b) 上のモデルでは`log(sigma)`がそうですが、ループするバージョンでは繰り返される計算が、1度計算された後は再利用されるようになります。

### 6.2. 係数とスケールの事前分布

この節では、回帰の係数とスケールの事前分布をモデリングするときにどのような選択肢があるか説明します。階層モデルにおける1変量のパラメータの事前分布は6.9節で議論し、多変量のパラメータについては6.12節で議論します。また、モデルの識別性のために使う事前分布については6.11節で議論します。

#### 背景となる文献

スケールパラメータの事前分布の選択についての概要についてさらに知るにはGelman (2006)を参照してください。罰則付き最尤推定値におけるスケールの事前分布の選択の概要についてはChung et al. (2013)を参照してください。回帰係数の事前分布の選択に関する議論についてはGelman et al. (2008)を参照してください。

#### 非正則一様事前分布

Stanはデフォルトでは、宣言された制約で決まる値をすべて取りうる一様（あるいは「平坦」）事前分布をパラメータに設定します。したがって、制約なしに宣言されたパラメータはデフォルトでは($-\infty$,$\infty$)の一様事前分布が与えられます。一方、下限が0と宣言されたスケールパラメータでは、(0,$\infty$)の非正則一様事前分布となります。両者の事前分布とも、取りうる範囲全体で積分すると1になるような密度関数には決して定式化されないという意味で非正則です。

Stanでは、非正則事前分布でモデルを定式化することができますが、サンプリングあるいは最適化がうまくいくためには、事後分布が正則になるようなデータを与える必要があります。これには通常、最小量のデータが必要となりますが、推定の出発点としては有用なことがあります。あるいは、感度分析（すなわち、事前分布が事後分布に及ぼす影響を検討する）の起点としてもそうです。

一様事前分布は、定式化された軸に特異的です。例えば、スケールパラメータ$\sigma > 0$に(0,$\infty$)という一様事前分布を与え、$q(\sigma)=c$（「密度」は正規化されていないものだけではなく、正規化されない可能性のあるものにも使われるので、ここでは$q$を使います）とすることもできるでしょうし、対数軸を使って、$\log\sigma$に($-\infty$,$\infty$)という一様事前分布を与え、$q(\log\sigma)=c$とすることもできるでしょう。対数変換に必要なヤコビアンの調整のため、これらは$\sigma$について別の事前分布となります。変数の変化とそれに必要なヤコビアンの調整についてもっと知りたいときは56.1節を参照してください。

Stanは、制約付きで宣言された変数について、制約の値を取りうる一様密度となるように、必要なヤコビアンの調整を自動的におこないます。このヤコビアンの調整は、最尤推定値を適切なものとするため、最適化のときには行なわれません。

#### 正則一様事前分布: 範囲の制約

上限と下限の両方を設定することで、正則一様事前分布を取るような変数を宣言することも可能です。以下はその例です。

```
real<lower=0.1, upper=2.7> sigma;
```

これは`sigma`に、$\mathsf{Uniform}(0.1,2.7)$という事前分布を暗黙のうちに与えるでしょう。

##### 制約を合わせる

制約すべてについて同じですが、`sigma`の取りうる値すべてについてモデルでも同様となっていることが重要です。例えば、下のコードでは`sigma`が正に制約されていますが、その一様事前分布では上下限を設定しています。

```
parameters {
  real<lower=0> sigma;
  ...
model {
  // *** 悪い例 *** : 制約より狭い範囲しか取りません
  sigma ~ uniform(0.1, 2.7);
```

このサンプリング文は`sigma`に(0.1, 2.7)の範囲を設定しています。この範囲は、制約で宣言された範囲、すなわち(0, $\infty$)よりも狭くなっています。このようにすると、Stanプログラムは、初期化が困難になったり、サンプリング中にハングしたり、ランダムウォークに陥ったりする可能性があります。

##### 上下限付近の推定値

範囲制限のついたパラメータの境界近くに推定値があることは通常、事前分布がモデルに合っていないことを示しています。また、サンプリングまたは最適化の際に、アンダーフローやオーバーフローといった数値的な問題を起こす可能性もあります。

#### 「無情報」正則事前分布

回帰係数に$\mathsf{Normal}(0,1000)$のような事前分布をつけたモデルは珍しくありません。<sup>2</sup>事前分布のスケールが、1000のように、推定される係数よりも数桁大きい場合は、そのような事前分布は事実上まったく何の効果も持ちません。

下のような、BUGSの例題(Lunn et al., 2012)全体で提案されている、スケールについてのデフォルトの事前分布は使わないようにしましょう。

![$$ \sigma^2 \sim \mathsf{InvGamma}(0.001, 0.001) $$](fig/fig04.png)

そのような事前分布は、妥当な事後分布の値の外側に、あまりに大きく集中した確率の山があり、正規分布につける対称的で広い事前分布とは異なり、事後分布をゆがめる深刻な影響が発生する可能性があります。例題と議論はGelman (2006)を参照してください。

<sup>2</sup>この習慣はBUGSでは普通で、例題(Lunn et al., 2012)のほとんどに見受けられます。

#### 切断事前分布

下限が0として宣言された変数に、正規分布の事前分布を設定すると、Stanのモデルでは、正則に切断された半正規分布の事前分布を設定するのと同じ効果も持ちます。0での切断分布を指定する必要はありません（**訳：「切断分布」でよい？**）。Stanで必要なのは、密度から割合までだけだからです。そこで、以下のように変数を宣言します。

```
real<lower=0> sigma;
```

そして、事前分布を与えます。

```
sigma ~ normal(0,1000);
```

すると、`sigma`には半正規分布の事前分布が与えられます。技術的には以下のようになります。

![$$ p(\sigma) = \frac{\mathsf{Normal}(\sigma \mid 0,1000)}{1 - \mathsf{NormalCDF}(0 \mid 0, 1000)} \propto \mathsf{Normal}(\sigma \mid 0, 1000) $$](fig/fig05.png)

しかしStanでは、半正規分布を正規化するのに必要な、正規分布の累積分布関数(CDF: cumulative distribution function)の計算を避けることができます。もし、事前分布の位置あるいはスケールがパラメータであるか、あるいは切断点がパラメータであるならば、切断の計算をせずに済ますことはできません。（**訳：「切断の計算」でよい？**）その場合は、正規分布のCDF項が定数にならないからです。こうした情報から弱情報事前分布を構成できます。

#### 弱情報事前分布

普通は、推定される変数のスケールについて研究者は何らかの知識を持っているでしょう。例えば、成人女性の身長の母平均について切片だけのモデルを推定するなら、答えは1から3メートルの間のどこかにあるでしょう。

同様に、予測変数が標準スケール（おおよそ平均が0で、単位分散）のロジスティック回帰なら、係数の絶対値が5より大きくなることはあまりないでしょう。この場合、$\mathsf{Normal}(0,5)$のような弱情報事前分布をそのような係数に設定するのは道理にかなっています。

計算機的にも統計学的にも、推定を制御するのに弱情報事前分布は役に立ちます。計算機的には、解があると期待される量のまわりの曲率を増加させます。これにより、L-BFGSのような勾配に基づく方法でもハミルトニアンモンテカルロのサンプリングでも、曲面（**訳:'surface'は「曲面」でよい？**）の位置から遠く離れすぎたところに迷い込まないようになります。統計学的には、女性の平均身長のような問題で弱情報事前分布はより有効です。というのも、$\mathsf{Normal}(0,1000)$のような非常に幅の広い事前分布では、事前分布の確率質量の大半が、期待される答えの範囲外にあるようにされるからです。小さなデータセットを使う推定では、そうした事前分布の方が圧倒的になることがありえます。

#### 上下限のある事前分布

女性の身長の例についてもう一度考えてみましょう。正則事前分布を正式化する方法の1つは、上下限のあるスケールに一様事前分布を設定することです。例えば、女性の平均身長のパラメータは、下限が1メートルで上限が3メートルと宣言することができるでしょう。確かに答えはこの間にあるはずです。

同様に、スケールパラメータの事前分布の下限に0を、上限に、10,000のような非常に大きな数を設定する例を見ることも珍しくありません。<sup>3</sup>これは、幅の広い逆ガンマ分布を分散の事前分布に与えて推定するのと、おおまかに言って同じ問題をもたらします。物理的に完全に制約があるというわけではないパラメーターは固定せずに、情報事前分布を設定する方がよいでしょう。女性の身長の場合なら、そのような事前分布は、メートルのスケールで$\mathsf{Normal}(2,0.5)$のようになると思われます。この場合、(1,3)の区間に確率質量の95%が集中しますが、依然としてその範囲外の値も取りえます。

上下限のある事前分布を使う場合は、パラメータの推定値が上下限に、あるいはそれに非常に近い値になっていないか、当てはめた事後分布を確認すべきです。そうなっていることは、計算上の問題が発生しているだけではなく、モデルの定式化に問題があることを示しています。そのような場合、設定した制約がないときにパラメータが当てはまると思われるところまで範囲を広げるか、あるいは上下限を避ける事前分布を使うべきです（6.9節を参照）。

<sup>3</sup> これもBUGSの例題モデル(Lunn et al., 2012)でよくある戦略でした。もう1歩進めて、数値的に0へとアンダーフローするのを防ぐために下限に0.001のような小さい数を設定することもよくありました。

#### 裾の重い事前分布と「デフォルト」の事前分布

外れ値に対応したいときの合理的な選択肢は、取りうる値と期待される範囲のあたりに確率質量の大半が集中するものの、裾にもかなりの確率質量がまだ残るような事前分布を使うことです。このような状況では通常、コーシー分布を事前分布に使うことが選択されます。そうすれば、中央値のまわりに確率質量を集中させることができますが、分散が無限大になるくらいに裾が重くなります。

とくに情報がないのであれば、回帰係数のパラメータにはコーシー分布の事前分布がデフォルトとして非常に良い選択ですし(Gelman et al., 2008)、スケールパラメータには半コーシー分布（Stanでは暗黙的にコードされます）がデフォルトとして良い選択です(Gelman, 2006)。

#### 情報事前分布

理想的な場合には、問題についての実質的な情報があって、弱情報事前分布よりもいっそう強い事前分布を含めることができるでしょう。これは、事前に実際に実験をして、別のデータの事後分布として得られるかもしれませんし、メタ解析から得られるかもしれませんし、単に分野の専門家から求められたというものかもしれません。より強度になったというだけで、弱情報事前分布の長所はすべて当てはまります。

#### 共役性

ギブズサンプリングとは異なり、共役事前分布（すなわち、事後分布が同族となるような事前分布）を設定する計算上の利点はStanのプログラムにはありません。<sup>4</sup>ハミルトニアンモンテカルロではサンプリングも最適化も共役性を使っていません。対数密度とその派生量だけで動作します。

<sup>4</sup> BUGSとJAGSはともに、ギブズサンプリングによる共役サンプリングをサポートしています。JAGSは共役の範囲を拡張しており、GLMモジュールで利用できます。Stanとは異なり、BUGSとJAGSはともに、共分散行列や単体のような多変量の量については共役事前分布を使うように制限されています。

### 6.3. ロバストノイズモデル

線形解析の標準的な手法では、ノイズ項$\epsilon$が正規分布するとモデリングします。Stanの視点から言うと、正規分布のノイズは特別なものではありません。例えば、ノイズ項にスチューデントのt分布を与えるとロバスト回帰に対応できます。Stanでコーディングするには、サンプリングの分布を以下のように変えます。

```
data {
  ...
  real<lower=0> nu;
}
...
model {
  for (n in 1:N)
    y[n] ~ student_t(nu, alpha + beta * x[n], sigma);
}
```

自由度の定数`nu`はデータとして指定します。

### 6.4. ロジスティック回帰とプロビット回帰

結果が2値のときは、ロジスティック回帰とプロビット回帰という近い関係にあるモデルのいずれかが使われるでしょう。一般化線形モデルとしては両者は、($-infty$,$infty$)という線形軸の予測値を、(0,1)という確率の値へと対応させるリンク関数だけが異なります。リンク関数はそれぞれ、ロジスティック関数と標準正規累積分布関数で、ともにシグモイド関数（すなわり、両者ともS字型）です。

予測変数1つと切片とからなるロジスティック回帰モデルは以下のようにコーディングされます。

```
data {
  int<lower=0> N;
  vector[N] x;
  int<lower=0,upper=1> y[N];
}
parameters {
  real alpha;
  real beta;
}
model {
  y ~ bernoulli_logit(alpha + beta * x);
}
```

ノイズパラメータは、直接指定されるのではなく、ベルヌーイ分布の定式化に組み込まれています。

ロジスティック回帰は一般化線形モデルの1種で、結果が2値で、リンク関数が対数オッズ（ロジット）関数です。これは以下のように定義されます。

![$$ \mathrm{logit}(\nu) = \log\left(\frac{\nu}{1 - \nu}\right) $$](fig/fig06.png)

リンク関数の逆関数がモデル中にあります。

![$$ \mathrm{logit}^{-1}(u) = \frac{1}{1 + \exp(-u)} $$](fig/fig07.png)

上のモデルの定式化では、ベルヌーイ分布をロジットでパラメータ化したバージョンを使っています。その定義は以下です。

![$$ \mathsf{BernoulliLogit}(y \mid \alpha) = \mathsf{Bernoulli}(y \mid \mathrm{logit}^{-1}(\alpha)) $$](fig/fig08.png)

この定式化ではまたベクトル化もおこなわれています。`alpha`と`beta`とがスカラーで`x`がベクトルなので、`alpha + beta * x`はベクトルになるからです。このベクトル化された定式化は、下のもっと非効率なバージョンと等価です。

```
for (n in 1:N)
  y[n] ~ bernoulli_logit(alpha + beta * x[n]);
```

ロジットのベルヌーイ分布の部分を展開すると、モデルは等価なまま、より明示的になりますが、より非効率で、算術的により不安定になります。

```
for (n in 1:N)
  y[n] ~ bernoulli(inv_logit(alpha + beta * x[n]));
```

別のリンク関数も同様に使えるでしょう。例えば、プロビット回帰は、正規累積分布関数を使います。これは以下のように書かれます。

![$$ \Phi(x) = \int_{-\infty}^{\infty}\mathsf{Normal}(\gamma \mid 0, 1)dy $$](fig/fig09.png)

標準正規累積分布関数$\Phi$は、Stanでは`Phi`関数として実装されています。Stanでプロビット回帰モデルをコーディングするには、ロジスティックモデルのサンプリング文を以下のように変えればよいでしょう。

```
y[n] ~ bernoulli(Phi(alpha + beta * x[n]));
```

Stanでは、標準正規累積分布関数$\Phi$の高速な近似が`Phi_approx`関数として実装されています。近似プロビット回帰モデルは以下のようにコーディングされるでしょう。

```
y[n] ~ bernoulli(Phi_approx(alpha + beta * x[n]));
```

### 6.5. 多項ロジット回帰

ロジスティック回帰の形式で、結果が多値になるものもStanでそのままコーディングできます。例えば、それぞれの出力の変数$y_{n}$について、結果が$K$種類の値を取りうるとします。また、$y_{n}$について、予測変数の$D$次元ベクトル$x_{n}$があるとします。係数の事前分布を$\mathsf{Normal}(0,5)$とした多項ロジットモデルは以下のようにコーディングされます。

```
data { int K;
  int N;
  int D;
  int y[N];
  vector[D] x[N];
}
parameters {
  matrix[K,D] beta;
}
model {
  for (k in 1:K)
    beta[k] ~ normal(0,5);
  for (n in 1:N)
    y[n] ~ categorical(softmax(beta * x[n]));
}
```

`softmax`関数の定義は34.11節を参照してください。最後の行をもっと効率的にすると以下のように書けます。

```
y[n] ~ categorical_logit(beta * x[n]);
```

`categorical_logit`分布は、カテゴリカル分布に似ていますが、パラメータがロジットスケールになります（`categorical_logit`の完全な定義は39.5節を参照してください）。

最初のループをもっと効率的にするには、行列`beta`をベクトルに変換して最初のループをベクトル化するとよいでしょう。

```
to_vector(beta) ~ normal(0,5);
```

##### データ宣言時の制約

上のモデルの`data`ブロックは、サイズ`K`, `N`, `D`や結果の配列`y`に制約をつけず定義しています。データ宣言での制約は、データが読み込まれた時点での（あるいは変換データが定義された時点での）エラーチェックのためのもので、サンプリングが始まる前に行なわれます。データ宣言での制約はまた、モデル作者の意図をより明示的に示すもので、可読性を高めることもあります。上のモデルの宣言をもっと厳しくするとすると以下のようになります。

```
int<lower=2> K;
  int<lower=0> N;
  int<lower=1> D;
  int<lower=1,upper=K> y[N];
```

これら制約の根拠ですが、カテゴリーの数`K`は、カテゴリカル分布が使えるためには少なくとも2でなくてはなりません。データ項目の数`N`は0となりえますが、負ではいけません。Rとは違ってStanの`for`ループは常に前進するので、`1:N`というループの範囲は、`N`が0に等しいときにはループの内部を実行しないことを保証します。予測変数の数`D`は、`beta * x[n]`が`softmax()`に適切な引数を生成するよう、少なくとも1でなくてはいけません。カテゴリカル分布の結果`y[n]`は、離散サンプリングがうまく定義されるように`1`から`K`までになくてはなりません。

データ宣言の制約は任意です。一方、`parameters`ブロックで宣言されるパラメータの制約は任意**ではありません**。すべてのパラメータの値が制約を確実に満たすようにすることが求められます。`transformed data`, `transformed parameters`, `generated quantities`での制約も任意です。

#### 識別可能性

入力の各成分に定数を加えてもsoftmaxは不変ですので、このモデルは典型的には、係数についてうまい事前分布があるときにだけ識別されます。

別の選択肢は、$(K-1)$次元のベクトルを使って、そのうちのひとつを0に固定することです。8.2節で、ベクトル中に定数とパラメータとを混在させる方法を議論します。多項ロジットの場合は、`parameters`ブロックは、$(K-1)$次元ベクトルを使って以下のように再定義されるでしょう。

```
parameters {
  matrix[K - 1, D] beta_raw;
}
```

それから、モデルで使うパラメータに変換します。まず、`transformed data`ブロックを`parameters`ブロックの前に加えて、零値からなる列ベクトルを定義します。

```
transformed data {
  vector[D] zeros;
  zeros <- rep_vector(0, D);
}
```

つづいて、これを`beta_row`に付け加えて、係数行列`beta`をつくります。

```
transformed parameters {
  matrix[K, D] beta;
  beta <- append_col(beta_raw, zeros);
}
```

`rep_vector`の定義は34.7節を、`append_col`の定義は34.10節を参照してください。

これは、パラメータとして$K$次元ベクトルを使ったモデルとまったく同じというわけではありません。事前分布が$(K-1)$ベクトルにのみ適用されるようになったからです。実用的には、これにより最尤法の解が違うものになり、事前分布を0のまわりに中央化すると事後分布もやや変わります。これは回帰係数でよく起こります。(**訳:ここは不安**)

### 6.6. 中央化ベクトルへのパラメータ化

パラメータベクトル$\beta$を、合計して0になる制約を満たすように中央化すると都合が良いことがよくあります。

![$$ \sum_{k=1}^{K}\beta_{k} = 0 $$](fig/fig10.png)

このようなパラメータベクトルは、多項ロジット回帰のパラメータベクトルを識別するのに使われたり（6.5節を参照）、IRTモデルの能力パラメータや困難度パラメータ（ただしどちらか一方）に使われることがあります（6.10節を参照）。

#### K-1自由度

合計して0になる制約をパラメータベクトルに課す方法は1つだけではありません。もっとも効率的なのは、1から$K-1$番目までの合計の符号を反転させたものとして$K$番目の要素を定義する方法です。

```
parameters {
  vector[K-1] beta_raw;
  ...
transformed parameters {
  vector[K] beta;  // centered
  for (k in 1:(K-1)) {
    beta[k] <- beta_raw[k];
  }
  beta[K] <- -sum(beta_raw);
  ...
```

このパラメータ化で`beta_row`に事前分布を置くと、合計して0の制約をつけずに元のパラメータ化で`beta`に同じ事前分布を置いたものと比較して、事後分布はわずかに異なることになります。とくに、`beta_raw`の各要素にに単純な事前分布を設定すると、制約のない$K$次元ベクトル`beta`の各要素に同じ事前分布を設定したのと異なる結果が得られます。たとえば、`beta`の事前分布に$\mathsf{Normal}(0,5)$を設定すると、`beta_raw`に同じ事前分布を設定したものと事後分布は異なることになるでしょう。

#### 単体への移動およびスケール変換

もう1つの方法は効率の点では劣りますが、対称な事前分布に改良できます。オフセットをつけてスケールを合わせることにより単体とする方法です。

```
parameters {
  simplex[K] beta_raw;
  real beta_scale;
  ...
transformed parameters {
  vector[K] beta;
  beta <- beta_scale * (beta_raw - 1.0 / K);
  ...
```

`beta_raw`は単体なので、合計すると1になります。これにより、要素ごとに$1/K$を減じると合計が0になることが保証されます（整数演算により0に丸められるのを防ぐため、`1 / K`ではなく、`1.0 / K`という式を使うことに注意しましょう）。単体の要素の大きさには限度がありますから、`beta`が、合計して0となるようなあらゆる値を取るために必要な自由度$K$を持つようにするためにはスケーリング因子が必要になります。

このパラメータ化では、ディリクレ分布の事前分布を`beta_raw`に置くことができます。おそらくは一様分布でしょうが。また、別の事前分布を`beta_scale`に設定します。通常は「縮小」となります。

#### 弱い中央化

$\beta \sim \mathsf{Normal}(0,\sigma)$のような事前分布を加えることは、パラメータのベクトル$\beta$について、$\sum_{k=1}^{K}\beta_{k}=0$となることはないものの、それに近くなるような、1種の弱い中央化を設定することになるでしょう。スカラーの定数$c$についての要素ごとの和$\beta+c$と$\beta$とが同じ尤度を生成する場合のみ、この方法でおおまかに中央化されることが保証されます（IRTモデルではおそらく、別のベクトル$\alpha$があって、$\alpha-c$と変換されます）。これは、対称な事前分布を得るためのまた別の方法です。

### 6.7. 順序ロジスティック回帰と順序プロビット回帰

予測変数$x_{n} \in \mathbb{R}^{D}$に対する結果$y_{n} \in {1,\dots,K}$の順序回帰は、単一の係数ベクトル$\beta \in \mathbb{R}^{D}$と、切断点の列$c \in \mathbb{R}^{K-1}$により決まります。ただし$c$は、$c_{d} < c_{d+1}$のように並んでいます。線形予測子$x_{n}\beta$が$c_{k-1}$と$c_{k}$の間に入るなら、離散値の結果は$k$となります。ここで、$c_{0} = -\infty$かつ$c_{K} = \infty$と仮定されています。ノイズ項は回帰の形式によって決まります。ここでは、回帰ロジスティック回帰と回帰プロビット回帰の例を示します。

#### 順序ロジスティック回帰

順序ロジスティック回帰はStanでは、切断点に`ordered`データ型を使い、組込みの`ordered_logistic`分布によりコーディングできます。

```
data {
  int<lower=2> K;
  int<lower=0> N;
  int<lower=1> D;
  int<lower=1,upper=K> y[N];
  row_vector[D] x[N];
}
parameters {
  vector[D] beta;
  ordered[K-1] c;
}
model {
  for (n in 1:N)
    y[n] ~ ordered_logistic(x[n] * beta, c);
}
```

切断点`c`のベクトルは`ordered[K-1]`として宣言します。これにより、`c[k]`が`c[k+1]`よりも小さいことが保証されます。

切断点に独立に事前分布を割り当てれば、この制約は、順序の制約を満たす点となるようにうまく同時事前分布を切断します。幸運にもStanでは、確率は割合で分かればよいので、正規化項についての制約の効果を計算する必要がありません。

##### 順序プロビット

順序プロビットモデルは、累積ロジスティック分布(`inv_logit`)を累積正規分布(`Phi`)に変えるだけで、まったく同様にコーディングできるでしょう。

```
data {
  int<lower=2> K;
  int<lower=0> N;
  int<lower=1> D;
  int<lower=1,upper=K> y[N];
  row_vector[D] x[N];
}
parameters {
  vector[D] beta;
  ordered[K-1] c;
}
model {
  vector[K] theta;
  for (n in 1:N) {
    real eta;
    eta <- x[n] * beta;
    theta[1] <- 1 - Phi(eta - c[1]);
    for (k in 2:(K-1))
      theta[k] <- Phi(eta - c[k-1]) - Phi(eta - c[k]);
    theta[K] <- Phi(eta - c[K-1]);
    y[n] ~ categorical(theta);
  }
}
```

ロジスティックモデルも、`Phi`を`inv_logit`に入れ替えれば、この方法でコーディングできるでしょう。ただし、単体の変換に基づく組込みのエンコーディングの方がより効率的で、数値的により安定です。`Phi(eta - c[k])`の値を一度だけ計算させて、その後は保存しておいた値を再利用するようにすると、少しだけ効率が良くなるでしょう。

### 6.8. 階層ロジスティック回帰

もっとも単純なマルチレベルモデルは、$L$だけある離散カテゴリー（あるいはレベル）にデータがグループ化されるような階層モデルです。極端な方法は、全データを完全にプールして、回帰係数$\beta$のベクトルを共通のものとして推定するというものでしょう。また反対側に極端な方法は、プールはせず、各レベル$l$に固有の係数ベクトル$\beta_{l}$を割り当て、他のレベルとは別に推定するというものでしょう。階層モデルは中間の解法で、プールの程度はデータと、プールの量についての事前分布で決まります。

2値の結果$y_{n} \in {0,1}$がそれぞれ、レベル$ll_{n} \in {1,\dots,L}$と関連しているとします。各結果はまた、予測変数のベクトル$x_{n} \n \mathbb{R}^{D}$とも関連しているでしょう。各レベル$l$は、固有の係数ベクトル$\beta_{l} \in \mathbb{R}^{D}$をとります。階層モデルでは、これもデータから推定される事前分布から係数$\beta_{l,d} \in \mathbb{R}^{D}$が抽出されます。この、階層的に推定される事前分布がプールの量を決定します。各レベルのデータが非常に似ているなら、低い階層分散を反映して強いプールが行なわれるでしょう。レベル間でデータが違っていれば、高い階層分散を反映して弱いプールが行なわれるでしょう。

以下のモデルは、回帰係数に階層事前分布を設定した階層ロジスティック回帰モデルをコーディングしたものです。

```
data {
  int<lower=1> D;
  int<lower=0> N;
  int<lower=1> L;
  int<lower=0,upper=1> y[N];
  int<lower=1,upper=L> ll[N];
  row_vector[D] x[N];
}
parameters {
  real mu[D];
  real<lower=0> sigma[D];
  vector[D] beta[L];
} model {
  for (d in 1:D) {
    mu[d] ~ normal(0,100);
    for (l in 1:L)
      beta[l,d] ~ normal(mu[d],sigma[d]);
  }
  for (n in 1:N)
    y[n] ~ bernoulli(inv_logit(x[n] * beta[ll[n]]));
}
```

標準偏差のパラメータ`sigma`は、下限値0の制約をつけて宣言されているので、暗黙の一様事前分布$(0, \infty)$を取ります。Stanでは、事後分布が正則である限り、非正則の事前分布を許します。とはいっても通常は、すべてのパラメータに情報事前分布、あるいは少なくとも弱情報事前分布をつけるのが有用です。回帰係数とスケールの事前分布についてのおすすめは6.2節を参照してください。

##### モデルの最適化

可能なところでは、サンプリング文をベクトル化すると、対数確率と導出量の評価が速くなります。高速化するのは、ループがなくなったからではなく、対数確率と勾配計算の下位計算がベクトル化により共有されること、勾配計算に必要な式木のサイズが減少することによります。

まず最初の最適化として、`D`についての`for`ループをベクトル化します。

```
mu ~ normal(0,100);
  for (l in 1:L)
    beta[l] ~ normal(mu,sigma);
```

`beta`はベクトルの配列として宣言されていますので、式`beta[l]`はベクトルを示すことになります。`beta`を行列として宣言することもできたでしょうが、ベクトルの配列（あるいは2次元配列）の方が行へのアクセスはより効率的です。配列、ベクトル、行列の間での効率性のトレードオフについては第4章にさらに情報があります。

このモデルは、ベルヌーイ分布内で逆ロジットを使用しているのを、ロジットでパラメータ化したベルヌーイ分布に置き換えることで、さらに高速化し、算術的にもより安定させることができます。

```
for (n in 1:N)
  y[n] ~ bernoulli_logit(x[n] * beta[ll[n]]);
```

`bernoulli_logit`の定義は38.2節を参照してください。

RやBUGSとは異なり、ループや、配列へのアクセスおよび代入は、Stanでは直接C++に変換されるので高速です。ほとんどの場合、コンテナに配置したり代入したりするコストは、対数確率と勾配の計算をベクトル化することによる効率の増加でおつりが来ます。ですので、サンプリング文をループさせていた元の定式化よりも下のバージョンの方が高速です。

```
{
  vector[N] x_beta_ll;
  for (n in 1:N)
    x_beta_ll[n] <- x[n] * beta[ll[n]];
  y ~ bernoulli_logit(x_beta_ll);
}
```

局所変数`x_beta_ll`のため、中括弧で、新しいスコープを導入しています。あるいは代わりに、`model`ブロックの最初で変数を宣言しても構いません。

上のように局所変数への代入を使うと、モデルが読みにくくなる場合があります。そのような場合には、まず読みやすいバージョンでモデルを開発、デバッグし、単純な定式化でデバッグし終わってはじめて最適化の作業にかかるというようにするのがおすすめです。

### 6.9. 階層事前分布

事前分布の事前分布は「超事前分布」とも呼ばれます。下位レベルのパラメータの事前分布では、事前分布の情報と同じだけの情報が利用可能ですが、超事前分布もそれが使われるのと同様に扱われるはずです。超事前分布は、ほんの少数の下位レベルのパラメータにしか適用されないことが多いので、事後分布が正則であることと、事前分布の裾が広くなることに対して統計的にも計算的にも過度に敏感ではないことを確かであることに注意する必要があります。（**訳:ここはよくわかりませんでした。**）

#### 階層モデルのMLEで、限度がなくなる事前分布

階層モデルの設定における最尤推定(MLE)の基本的な問題は、階層分散が小さくなって、階層平均のまわりに値が集まり、全体の密度が限度なく大きくなることです。例として、$x_{n} \n \mathbb{R}^{K}$についての$y_{n} \in \mathbb{R}$の単純な階層線形回帰（事前分布の平均を固定）を考えます。定式化は以下のとおりです。

![$$ \begin{array}{rl}y_{n} &\sim \mathsf{Normal}(x_{n}\beta, \sigma)\\ \beta_{k} &\sim \mathsf{Normal}(0, \tau)\\ \tau &\sim \mathsf{Cauchy}(0, 2.5) \end{array} $$](fig/fig11.png)

この場合で、$\tau \rightarrow 0$かつ$\beta_{k} \rightarrow 0$となるとき、事後密度

![$$ p(\beta, \tau, \sigma \mid y, x) \propto p(y \mid x, \beta, \tau, \sigma) $$](fig/fig12.png)

は限度なく大きくなります。図21.1にNealのじょうご密度の図がありますが、これと同様の挙動を示します。

この場合明らかに、$\beta$、$\tau$、$\sigma$に最尤推定値はありません。したがって、事後の最頻値を推測に使うなら、モデルを変えなければなりません。Chung et al. (2013)の推奨は、以下のように事前分布にガンマ分布を使用する方法です。

![$$ \sigma \sim \mathsf{Gamma}(2, 1/A) $$](fig/fig13.png)

$A$には、$A=10$のようにかなり大きな値を与えます。

### 6.10. 項目反応理論モデル

項目反応理論(Item-response theory: IRT)は、何人かの生徒がそれぞれ、1つ以上のテスト問題群に答えるという状況をモデリングします。このモデルは、生徒の能力と、問題の難しさとについてのパラメータに基づいています。よりはっきりしたモデルでは、問題の識別性と、当て推量で正解になる確率についてのパラメータもあります。階層IRTモデルの入門用教科書にはGelman and Hill (2007)を参照してください。また、さまざまなIRTモデルのBUGSによるコーディングについてはCurtis (2010)を参照してください。

#### 欠測のあるデータ宣言

IRTモデルに渡されるデータは、全生徒が全問題に解答するとは限らないことを考慮すると、以下のように宣言されるでしょう。

```
data {
  int<lower=1> J;              // 生徒の数
  int<lower=1> K;              // 問題の数
  int<lower=1> N;              // 観測の数
  int<lower=1,upper=J> jj[N];  // 観測nの生徒
  int<lower=1,upper=K> kk[N];  // 観測nの問題
  int<lower=0,upper=1> y[N];   // 観測nの正誤
}
```

この宣言では、生徒-問題の組み合わせが全部で`N`あり、`1:N`の各`n`が、2値の観測値`y[n]`のインデックスで、`y[n]`は、生徒`jj[n]`が問題`kk[n]`に正答したかどうかを示します。

ハイパーパラメータの事前分布は、この節の後では簡単のためハードコーディングします。とはいえ、Stanではもっと柔軟にデータとしてコーディングできます。

#### 1PL (Rasch) モデル

1PL項目反応モデルは、Raschモデルともいわれ、問題についての1つのパラメータ(1P)を持ち、ロジスティックリンク関数(L)を使います。

モデルのパラメータは以下のように宣言されます。

```
parameters {
  real delta;          // 平均の生徒の能力
  real alpha[J];       // 生徒jの能力 - 平均能力
  real beta[K];        // 問題kの難しさ
}
```

パラメータ`alpha[j]`は生徒`j`の能力の係数、`beta[k]`は問題`k`の難しさの係数です。ここで使われている非標準のパラメタライゼーションでは、切片項`delta`も含みます。これは、平均的な生徒の平均的な問題への反応をあらわします。<sup>5</sup>モデル自体は以下のようになります。

```
model {
  alpha ~ normal(0,1);      // 真値の情報事前分布
  beta ~ normal(0,1);       // 真値の情報事前分布
  delta ~ normal(.75,1);    // 真値の情報事前分布
  for (n in 1:N)
    y[n] ~ bernoulli_logit(alpha[jj[n]] - beta[kk[n]] + delta);
}
```

このモデルはロジットでパラメタライズしたベルヌーイ分布を使っています。

![$$ \mathsf{bernoulli\_logit}(y \mid \alpha) = \mathsf{bernoulli}(y \mid \mathsf{logit}^{-1}(\alpha)) $$](fig/fig14.png)

このモデルを理解する鍵は、`bernoulli_logit`分布の中の項です。以下の式に従います。

![$$ \Pr[y_{n} = 1] = \mathrm{logit}^{-1}(\alpha_{jj[n]} - \beta_{kk[n]} + \delta) $$](fig/fig15.png)

このモデルでは、事前分布を設定しないと加法的な識別可能性の問題が発生します。例えば、$\alpha_{j}$と$\beta_{k}$の両方に$\xi$を加えると、予測値が同じになります。$\alpha$と$\beta$に位置0（** ``located at 0''の訳はこれでよい? **）の事前分布を設定すると、パラメータが識別できます。識別可能性の問題と、識別可能にする他の手法についてはGelman and Hill (2007)を参照してください。

テスト用に、Stanとともに配布されているIRT 1PLモデルは、Rでデータをシミュレートするために使われる実際のデータ生成過程に合うような情報事前分布を使っています（このシミュレーションのコードはモデルと同じディレクトリに入っています）。実際に利用するにはほとんどの場合は現実的ではありませんが、Stanの推定が有効であることは分かります。事前分布の幅を広くして簡単な感度分析を行なうと、400人の生徒に100問の問題で、25%だけランダムに欠測するとしても、事後分布は事前分布にかなり敏感です。実用的には事前分布は、次の節で記述されるように、他のパラメータとともに階層的に合うようにすべきです。

<sup>5</sup> Gelman and Hill (2007)は$\delta$項を、生徒の能力の分布における位置のパラメータと等価に扱っています。

#### マルチレベル2PLモデル

前の節で記述した単純な1PLモデルをこの節では、問題にどのくらいのノイズがあるかをモデルする識別パラメータを加え、問題の難しさと識別パラメータにマルチレベルの事前分布を加えることで一般化します。モデルのパラメータは以下のように宣言されます。

```
parameters {
  real mu_beta;                // 平均の生徒の能力
  real alpha[J];               // jの能力 - 平均
  real beta[K];                // kの難しさ
  real<lower=0> gamma[K];      // kの識別性
  real<lower=0> sigma_beta;    // 難しさのスケール
  real<lower=0> sigma_gamma;   // 識別性のスケール
}
```

モデルの定義を見た後の方がパラメータはよくわかるでしょう。

```
model {
  alpha ~ normal(0,1);
  beta ~ normal(0,sigma_beta);
  gamma ~ lognormal(0,sigma_gamma);
  mu_beta ~ cauchy(0,5);
  sigma_alpha ~ cauchy(0,5);
  sigma_beta ~ cauchy(0,5);
  sigma_gamma ~ cauchy(0,5);
  for (n in 1:N)
    y[n] ~ bernoulli_logit(gamma[kk[n]]
                           * (alpha[jj[n]] - (beta[kk[n]] + mu_beta)));
}
```

1PLモデルに似ていますが、追加のパラメータ`gamma[k]`で、問題`k`の識別能力をモデリングしています。`gamma[k]`が1より大きければ、ランダムに正答になる可能性は低くなるので反応は減衰します。（** attenuated の訳?**）パラメータ`gamma[k]`は正値に制約されています。これは、能力が低い生徒の方により簡単な問題はないとするものです。そのような問題を耳にしないわけではありませんが、IRTモデルが利用されるような試験ではほとんどの場合、そのような問題は除かれる傾向にあります。

このモデルでは、生徒の能力`alpha`に標準正規分布の事前分布を与えるようにここではパラメタライズしています。これは、このパラメータの位置とスケールの両方を識別可能にするためです。さもなければ、その両方が識別不可能になるでしょう。識別可能性については20章でさらに議論します。難しさと識別性のパラメータ`beta`と`gamma`には弱情報事前分布を与えています。

```
beta ~ normal(0,5);
gamma ~ lognormal(0,2);
```

ポイントは、`alpha`がスケールと位置を決定し、`beta`と`gamma`は動けるようにしていることです。

パラメータ`beta`はここでは非中央化のパラメタライゼーションとしています。パラメータ`mu_beta`が`beta`の平均の位置を与えています。あるいはまた、以下のようにもできるでしょう。

```
beta ~ normal(mu_beta, sigma_beta);
```

および

```
y[n] ~ bernoulli_logit(gamma[kk[n]] * (alpha[jj[n]] - beta[kk[n]]));
```

非中央化のパラメタライゼーションは階層モデルではより効率的になる傾向にあります。非中央化のリパラメタライゼーションについてもっと知るには21.2節を参照してください。

切片の項`mu_beta`はそれ自身は階層的にモデリングで来ません。そのため、弱情報事前分布$\mathsf{Cauchy}(0,5)$を与えています。同様に、スケールの項`sigma_alpha`および`sigma_beta`、`sigma_gamma`には半コーシー分布の事前分布を与えています。半コーシー分布の切断は暗黙のものです。明示的な切断は必要ありません。これは、対数確率は割合だけ計算すれば良く、スケールの変数は宣言により(0,∞)に制約されているからです。

### 6.11. 識別可能性のための事前分布

#### 位置とスケールの固定

（階層）事前分布の応用の1つに、パラメータ群のスケールあるいは位置、またはその両方を識別させるということがあります。例えば、前の節で議論したIRTモデルでは、位置とスケールの両方に識別不能性がありました。一様事前分布をつけると、スケールと位置の両方の項でパラメータは固定されないでしょう。これが推定にもたらす問題については、20.1節に簡単な例がありますので、参照してください。

生徒の能力のような係数の1群に標準正規分布（すなわち$\mathsf{Normal}(0,1)$）の事前分布をを与えることで識別不能性は解決されます。生徒の能力に標準正規分布の事前分布をつけると、IRTモデルは識別され、生徒の能力のパラメータについての推定値の群が事後分布により生成され、その標本平均は0に近く、標本分散は1に近い値となるでしょう。すると、問題の難しさと識別性のパラメータには、広がった、理想的には階層事前分布を与えるべきで、生徒の能力のパラメータとの相対的な位置とスケールにより、これらパラメータは識別されるでしょう。

#### 共線性

事前分布により識別可能になる別の例として、線形回帰における共線性の場合があります。線形回帰では、2つの予測変数が共線的（すなわち一方が他方の線形関数となっている）ならば、それらの係数は、事後分布で相関係数が1（または-1)となります。これは識別不能になります。係数の事前分布を正規分布とすることで、2つの2重になった予測変数（自明に共線的）の最尤推定解（**maximum likelihood solution?**）は、1つだけを含むときに得られる値の半分になるでしょう。（**ここはよくわかりません。**）

#### 分離可能性

ロジスティック回帰では、結果が1で予測変数が正のときや、結果が0で予測変数が負のときには、こうした予測変数の係数の最尤推定値は無限に発散します。係数に事前分布を与えることでこうした発散を制御することができます。これにより、推定値を0に向けて「縮約」することで、事後分布についてモデルは識別可能になるでしょう。

同様の問題は非正則平坦事前分布からのサンプリングでも発生します。このときのサンプラーは非常に大きな値を抽出しようとするでしょう。事前分布を与えることで、事後分布は有限の値のまわりに集中し、サンプリングはうまくいくでしょう。

### 6.12. 階層モデルにおける多変量の事前分布

階層回帰モデルでは（他の状況でもありますが）、いくつかの個別レベルの変数に階層事前分布を割り当てることがあります。例えば、複数の変動する切片や傾きを含むモデルでは、多変量の事前分布を割り当てることでしょう。

例として、人を個別レベルとして、結果を収入、予測変数を教育水準と年齢とし、州などの地理区分を群としましょう。切片のほか、教育水準と年齢の効果も州ごとに変わりうるとします。さらに、州ごとの収入や失業水準の平均といった、州レベルの予測変数もあるとしましょう。

#### 多変量回帰の例

Gelman and Hill (2007)の13章・17章では、$N$個体が$J$群にまとめられる階層モデルについて議論しています。各個体は、大きさ$K$の列ベクトル$x_{n}$からなる予測変数を持ちます。記法の統一のため、$x_{n,1} = 1$として、固定された「切片」予測変数としています。群への所属をコード化するため、個体$n$は群$jj[n] \in 1:J$に属するとしています。各個体$n$はまた、実数値の観測結果$y_{n}$を持ちます。

##### 尤度

このモデルは、群によって切片と係数が変動する線形回帰となりますので、$\beta_{j}$は、群$j$についての$K$次元ベクトルの係数です。個体$n$についての尤度関数は以下のとおりです。

![$$ y_{n} \sim \mathsf{Normal}(x_{n}\beta_{jj[n]}, \sigma) \quad\text{ただし}\quad n \in 1:N $$](fig/fig16.png)

##### 係数の事前分布

GelmanとHillは、係数ベクトル$\beta_{j}$を、平均ベクトル$\mu$と共分散行列$\Sigma$を持つ多変量分布から抽出されるとモデリングしています。

![$$ \beta_{j} \sim \mathsf{MultiNormal}(\mu, \Sigma) \quad\text{ただし}\quad j \in 1:J $$](fig/fig17.png)

以下、GelmanとHillのフルモデルについて議論しますが、このモデルでは$\mu$をモデリングするのに群レベルの予測変数を使っています。ここでは、$\mu$は単純なベクトルのパラメータと仮定します。

##### ハイパーパラメータ

階層モデリングでは、群レベルの平均ベクトル$\mu$と共分散行列$\Sigma$自身に事前分布を与えなくてはなりません。群レベルの平均ベクトルには、独立した係数に合理的な弱情報事前分布を与えることができます。

![$$ \mu_{j} \sim \mathsf{Normal}(0, 5) $$](fig/fig18.png)

もちろん、係数$\beta_{j,k}$の期待値について知識があれば、その情報を$\mu_{k}$の事前分布に取り入れることができます。

共分散行列の事前分布についてGelmanとHillは、スケールをつけた（**?scaled?**）逆Wishart分布を使うことを勧めています。それを選ぶ理由は主として、多変量の尤度関数に対して共役であり、Gibbsサンプリングが単純になるという利便性にあります。

Stanでは、多変量の事前分布に共役の制限はありませんし、実際のところやや違った手法をお勧めしています。GelmanとHillと同様、事前分布をスケールと行列に分解しますが、実際の変数のスケールと相関行列に基づいた、もっと自然な方法でそのようにできます。特に以下のように定義します。

![$$ \Sigma = \mathrm{diag_matrix}(\tau)\Omega\mathrm{diag_matrix}(\tau) $$](fig/fig19.png)

ここで$\Omega$は相関行列で、$\tau$は係数のスケールのベクトルです。

スケールのベクトル$\tau$の要素には、スケールの事前分布に合理的ならどのようなものでも与えることができますが、下のような、小さいスケールの半コーシー分布のような弱情報のものをお勧めします。

![$$ \tau_{k} \sim \mathsf{Cauchy}(0, 2.5) \quad\text{ただし}\quad k \in 1:J \quad\text{かつ}\quad \tau_{k} > 0 $$](fig/fig20.png)

事前分布の平均については、群間の係数の変動のスケールについて情報があるなら、$\tau$の事前分布に取り入れるべきでしょう。交換可能な係数が多数あるときは、$\tau$自体の要素自体に（おそらく切片は除いて）階層事前分布が与えらえるでしょう。

最後に、相関行列$\Omega$には、形態$\nu > 1$のLKJ事前分布を与えることをお勧めします。

![$$ \Omega \sim \mathsf{LKJcorr}(\nu) $$](fig/fig21.png)

LKJ相関分布は51.1節で定義していますが、モデリングの基本的なアイデアは、$\nu$が大きくなるにつれ、事前分布が単位相関行列のまわりに集中するようになる（すなわち$\beta_{j}$の要素間の相関を低くする）ということです。$\nu=1$のとき、LKJ相関分布は、相関行列について同一分布にまで小さくなります。したがって、LKJ事前分布は、パラメータ$\beta_{j}$間の相関について期待する量を制御するのに用いられることがあります。

##### 事前分布の平均についての群レベルの予測変数

GelmanとHillのモデルを完成させるため、各群$j \in 1:J$には、群レベルの予測変数$u_{j}$の$L$次元の行ベクトルを与えるとします。すると$\beta_{j}$の事前分布の平均は、$L$次元の係数ベクトル$gamma$を使って、それ自体を回帰としてモデリングできます。群レベルの係数の事前分布は以下のようになります。

![$$ \beta_{j} \sim \mathsf{MultiNormal}(u_{j}\gamma, \Sigma) $$](fig/fig22.png)

群レベルの係数$gamma$には、それ自体に弱情報事前分布を独立に与えてもよいでしょう。

![$$ \gamma_{l} \sim \mathsf{Normal}(0, 5) $$](fig/fig23.png)

いつもと同様に、群レベルの平均についての情報は事前分布に取り入れるべきでしょう。

##### Stanでのモデルのコーディング

群レベルの係数についての多変量の事前分布と、群レベルの事前分布の平均を与えた完全な階層モデルのStanのコードは以下のように定義されます。

```
data {
  int<lower=0> N;              // 個体の数
  int<lower=1> K;              // 個体の予測変数の数
  int<lower=1> J;              // 群の数
  int<lower=1> L;              // 群の予測変数の数
  int<lower=1,upper=J> jj[N];  // 個体が属している群
  matrix[N,K] x;               // 個体の予測変数
  row_vector[L] u[J];          // 群の予測変数
  vector[N] y;                 // 結果
}
parameters {
  corr_matrix[K] Omega;        // 事前分布の相関
  vector<lower=0>[K] tau;      // 事前分布のスケール
  matrix[L,K] gamma;           // 群の係数
  vector[K] beta[J];           // 群ごとの個体の係数
  real<lower=0> sigma;         // 予測誤差のスケール
}
model {
  tau ~ cauchy(0,2.5);
  Omega ~ lkj_corr(2);
  to_vector(gamma) ~ normal(0, 5);
  {
    row_vector[K] u_gamma[J];
    for (j in 1:J)
      u_gamma[j] <- u[j] * gamma;
    beta ~ multi_normal(u_gamma, quad_form_diag(Omega, tau));
  }
  {
    vector[N] x_beta_jj;
    for (n in 1:N)
      x_beta_jj[n] <- x[n] * beta[jj[n]];
    y ~ normal(x_beta_jj, sigma);
  }
}
```

超事前分布の共分散行列は、2次形式でコード中で暗黙に定義されています。これは、相関行列`Omega`とスケールのベクトル`tau`が出力を調べるのにより自然だからです。`Sigma`を出力するためには、変換パラメータとして定義します。関数`quad_form_diag`は、`quad_form_diag(Sigma,tau)`が`diag_matrix(tau) * Sigma * diag_matrix(tau)`と等価になるように定義されています。ここで、`diag_matrix_(tau)`は、対角成分が`tau`となり、それ以外が0の行列を返します。ただし、`quad_form_diag`を使うバージョンの方が高速のはずです。特別な行列演算についてさらに知るには34.2節を参照してください。

##### ベクトル化による最適化

上のStanプログラムのコードでは、予測変数`x[n]`と群レベルの係数`beta[jj[n]]``を使って、`x_beta_jj[n]`という平均を記述する（**?entry?**）ベクトルを組み立てていました。`x_beta_jj`が局所変数として宣言できるように、その周りに中括弧でブロックを定義しました。このベクトル化したコードは、下の、より単純な、ベクトル化していないものと等価です。

```
for (n in 1:N)
  y[n] ~ normal(x[n] * beta[jj[n]], sigma);
```

上のStanプログラムのコードは結果と多変量正規分布のベクトルの配列も組み立てています。解いて微分する必要のある線形系の数を減らすことで、非常に顕著な高速化が可能になります。

```
{
  matrix[K,K] Sigma_beta;
  Sigma_beta <- quad_form_diag(Omega, tau);
  for (j in 1:J)
    beta[j] ~ multi_normal((u[j] * gamma)', Sigma_beta);
}
```

この例では、共分散行列`Sigma_beta`は、二次形式の計算を$J$回繰り返さなくても良いように局所変数として定義されています。このベクトル化は、次の節のコレスキー因子による最適化と組み合わせることもできます。

##### コレスキー分解による最適化

多変量正規密度と、相関行列に対するLKJ事前分布はともに、行列パラメータを因子分解する必要があります。これは、前の節のようにベクトル化すると、各密度について1度だけ行われることが保証されます。効率性の点でも数値的な安定性の点でももっと良い解法は、非中央化パラメタライゼーションの多変量版により、相関行列のコレスキー因子を直接使ってモデルをパラメタライズすることです。前の節のモデルについて、プログラム中の完全な行列の事前分布の部分を、等価なコレスキー分解した事前分布に置き換えると以下のようになります。

```
parameters {
  matrix[K,J] z;
  cholesky_factor_corr[K] L_Omega;
  ...
transformed parameters {
  matrix[J,K] beta;
  beta <- u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
}
model {
  to_vector(z) ~ normal(0,1);
  L_Omega ~ lkj_corr_cholesky(2);
  ...
```

新しいパラメータ`L_Omega`は、元の相関行列`Omega`のコレスキー因子です。つまり、以下のようになっています。

```
Omega = L_Omega * L_Omega'
```

事前分布のスケールのベクトル`tau`は変化ありません。さらに、コレスキー因子にスケールを左から掛けることで、最終的な共分散行列のコレスキー因子が生成されます。

```
Sigma_beta
= quad_form_diag(Omega,tau)
= diag_pre_multiply(tau,L_Omega) * diag_pre_multiply(tau,L_Omega)'
```

ここで、対角行列の、左から掛ける複合演算は以下のように定義されています。

```
diag_pre_multiply(a,b) = diag_matrix(a) * b
```

新しい変数`z`は行列として宣言されており、独立な標準正規分布の事前分布を与えられています。ここで、`to_vector`は行列をベクトルにする演算で、ベクトル化された引数に1変量の正規密度を与えるのに使うことができます。共分散行列のコレスキー因子に`z`を掛けて、平均`(u*gamma)'`を加えることで、元のモデルと同じ分布となる`beta`が生成されます。

データ宣言は省略しますが、前と同じモデルを最適化したものは以下のようになります。

```
parameters {
  matrix[K,J] z;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0>[K] tau;      // 事前分布のスケール
  matrix[L,K] gamma;           // 群の係数
  real<lower=0> sigma;         // 予測誤差のスケール
}
transformed parameters {
  matrix[J,K] beta;
  beta <- u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
}
model {
  vector[N] x_beta_jj;
  for (n in 1:N)
    x_beta_jj[n] <- x[n] * beta[jj[n]]';
  y ~ normal(x_beta_jj, sigma);
  tau ~ cauchy(0,2.5);
  to_vector(z) ~ normal(0,1);
  L_Omega ~ lkj_corr_cholesky(2);
  to_vector(gamma) ~ normal(0,5);
}
```

### 6.13. 予測、フォアキャストとバックキャスト

Stanのモデルは、モデル中の任意の未知量の値を「予測」するのに使うことができます。予測が将来についてなら「フォアキャスト」と呼ばれますし、気候の再現や宇宙論であるように、過去についてなら「バックキャスト」と呼ばれることがあります（「フォア」の対義語についての書き手の感覚により、「アフトキャスト」や「ハインドキャスト」、「アンテキャスト」とも呼ばれます）。

#### 予測のプログラミング

以下の線形回帰は単純な例ですが、まさに最初の例と同じ設定になっています。すなわち、`N`個の観測値`y`と、長さ`N`の予測変数のベクトル`x`を使って、係数`beta`を推定します。このモデルのパラメータと、観測値のモデルは前とまったく同じです。

予測のためには、予測の数`N_new`とその予測変数の行列`x_new`を与える必要があります。予測値自身はパラメータ`y_new`としてモデリングされています。新しい結果のベクトル`y_new`と予測変数の行列`x_new`がありますが、予測値のモデル文は観測値とまったく同じです。

```
data {
  int<lower=1> K;
  int<lower=0> N;
  matrix[N,K] x;
  vector[N] y;
  int<lower=0> N_new;
  matrix[N_new, K] x_new;
}
parameters {
  vector[K] beta;
  real<lower=0> sigma;

  vector[N_new] y_new;                  // 予測値
}
model {
  y ~ normal(x * beta, sigma);          // 観測モデル

  y_new ~ normal(x_new * beta, sigma);  // 予測モデル
}
```

#### 生成量としての予測

可能であれば、`generated quantities`ブロックを使うのが予測値を生成するのに最も効率的な方法です。これにより正則なモンテカルロ（マルコフ連鎖モンテカルロではありません）の推定値が得られます。これは、iterationあたりでのサンプルサイズをはるかに高効率にできます。

```
...上のデータ...

parameters {
  vector[K] beta;
  real<lower=0> sigma;
}
model {
  y ~ normal(x * beta, sigma);
}
generated quantities {
  vector[N_new] y_new;
  for (n in 1:N_new)
    y_new[n] <- normal_rng(x_new[n] * beta, sigma);
}
```

ここでは、データは前と同じですが、パラメータ`y_new`が新しく生成量(generated quantity)として宣言されています。また、予測モデルは`model`から取り除かれ、正規分布から擬似乱数を使って抽出されるようになっています。

### 6.14. 多変量の結果

ほとんどの回帰の設定では、1変量の観測値（スカラー値や論理値、カテゴリカル値、順序値、計数値など）をモデリングします。多項回帰も、カテゴリカル回帰の繰り返しにすぎません。これとは対照的に、各観測値が多変量の場合の回帰をこの節では議論します。複数の結果を回帰の設定で関連づけるために、それらの誤差項に共分散構造を与えます。

この節では2つの場合について考えます。連続の多変量の量についての見かけ上無関係な回帰と、論理値の多変量の量についての多変量プロビット回帰です。

#### 見かけ上無関係な回帰

最初に考えるモデルは、計量経済学の「見かけ上無関係な」回帰(seemingly unrelated regression: SUR)です。いくつかの線形回帰が予測変数を共有し、独立な誤差ではなく共分散誤差構造を使います(Zellner, 1962; Greene, 2010)。

このモデルは回帰として簡単に書けます。

![$$ \begin{array}{rl} y_{n} &= x_{n}\beta + \epsilon_{n}\\ \epsilon_{n} &\sim \mathsf{MultiNormal}(0, \Sigma) \end{array} $$](fig/fig24.png)

ここで、$x_{n}$は$J$次元の予測変数の行ベクトル（$x$は$(N \times J)$行列です）、$y_{n}$は$K$次元の観測値のベクトル、$\beta$は回帰係数の$(K \times J)$行列（ベクトル$\beta_{k}$は結果$k$についての係数を収容します）、$\Sigma$は誤差を決める共分散行列です。いつものように切片は、1の値の列として$x$の中に含めることができます。

基本的なStanのコードは単純です（とはいえ、相関に対してLKJ事前分布を使用してもっと最適化したコードがその下にあります）。


```
data {
  int<lower=1> K;
  int<lower=1> J;
  int<lower=0> N;
  vector[J] x[N];
  vector[K] y[N];
}
parameters {
  matrix[K,J] beta;
  cov_matrix[K] Sigma;
}
model {
  vector[K] mu[N];
  for (n in 1:N)
    mu[n] <- beta * x[n];
  y ~ multi_normal(mu, Sigma);
}
```

効率性のため、平均のベクトルの配列を前もって計算し、同じ共分散行列は共有して、多変量正規分布はベクトル化しています。

6.12節での助言にしたがって、回帰係数の事前分布には正規分布の弱情報事前分布を、相関にはLKJ事前分布を、標準偏差には半コーシー分布の事前分布を設定しましょう。共分散構造は、効率性と算術的な安定性のため、コレスキー因子によりパラメタライズされます。

```
...
parameters {
  matrix[K,J] beta;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0>[K] L_sigma;
}
model {
  vector[K] mu[N];
  matrix[K,K] L_Sigma;

  for (n in 1:N)
    mu[n] <- beta * x[n];

  L_Sigma <- diag_pre_multiply(L_sigma, L_Omega);

  to_vector(beta) ~ normal(0, 5);
  L_Omega ~ lkj_corr_cholesky(4);
  L_sigma ~ cauchy(0, 2.5);
  y ~ multi_normal_cholesky(mu, L_Sigma);
}
```

共分散行列のコレスキー因子はここでは局所変数として復元され、相関行列のコレスキー因子によりスケーリングされてモデルで使われています。行列`beta`をベクトルに変換することにより1度にまとめて回帰係数に事前分布を設定しています。

必要なら、`generated quantities`ブロックで、コレスキー因子から完全な相関または共分散行列を復元してもよいでしょう。

#### 多変量プロビット回帰

多変量プロビットモデルは、見かけ上無関係な回帰の結果にステップ関数を適用することにより一連の論理値の変数を生成します。

観測値$y_{n}$は、論理値（0を偽、1を真とコーディングします）からなる$D$次元ベクトルです。

観測値$y_{n}$は、見かけ上無関係な回帰モデル（前の節を参照）から抽出される潜在変数$z_{n}$にもとづいています。

![$$ \begin{array}{rl} z_{n} &= x_{n}\beta + \epsilon_{n}\\ \epsilon_{n} &\sim \mathsf{MultiNormal}(0, \Sigma) \end{array} $$](fig/fig25.png)

次に、ステップ関数を適用して、以下で定義される要素からなる論理値の$K$次元ベクトル$z_{n}$を生成します。

![$$ y_{n,k} = \mathrm{I}(z_{n,k} > 0) $$](fig/fig26.png)

ここで、I()は指示関数で、引数が真なら1の、そうでなければ0の値をとります。

見かけ上無関係な回帰モデルの場合とは異なり、ここでは共分散行列$\Sigma$は単位標準偏差をとります（すなわち、相関行列になります）。通常のプロビット回帰やロジスティック回帰と同様に、スケールが変動するとモデル（スケールではなく、0についての切断点によってのみ定義されています）が識別不能になります（Greene (2011)を参照）。

多変量プロビットモデルはStanでは、Albert and Chib (1993)によって導入されたトリックを使ってコーディングできます。この方法では、基礎となる連続値のベクトル$y_{n}$が切断パラメータとしてコーディングされます。Stanでのこのモデルのコーディングの鍵は、対応する$y$の値が0か1かにもとづいて、潜在ベクトル$z$を2つに分けて宣言することです。さもないと、このモデルは前の節の見かけ上無関係な回帰モデルと同じになってしまいます。

最初に、整数の2次元配列についての合計関数を導入します。これは、$y$の中にどれだけの1が合計してあるのかを計算するのに役立ちます。

```
functions {
  int sum(int[,] a) {
    int s;
    s <- 0;
    for (i in 1:size(a))
      s <- s + sum(a[i]);
    return s;
  }
}
```

この関数はちょっとしたことですが、Stanには組み込まれていませんし、気が散らないように自身の関数として定義しておくと（**?pulled into its own function?**）、このモデルの残りの部分をわかりやすくします。

データ宣言ブロックは、見かけ上無関係な回帰ととてもよく似ていますが、観測値`y`は今回は、0か1かに制約された整数です。

```
data {
  int<lower=1> K;
  int<lower=1> D;
  int<lower=0> N;
  int<lower=0,upper=1> y[N,D];
  vector[K] x[N];
}
```

データの宣言の後には`transformed data`ブロックがあります。これは、データの配列`y`を真か偽かの成分に分けるというただ1つの目的のために必要になったものです。これにより、`transformed parameters`ブロックで`z`が容易に復元できるようにインデックスの状態を把握します。

```
transformed data {
  int<lower=0> N_pos;
  int<lower=1,upper=N> n_pos[sum(y)];
  int<lower=1,upper=D> d_pos[size(n_pos)];
  int<lower=0> N_neg;
  int<lower=1,upper=N> n_neg[(N * D) - size(n_pos)];
  int<lower=1,upper=D> d_neg[size(n_neg)];

  N_pos <- size(n_pos);
  N_neg <- size(n_neg);
  {
    int i;
    int j;
    i <- 1;
    j <- 1;
    for (n in 1:N) {
      for (d in 1:D) {
        if (y[n,d] == 1) {
          n_pos[i] <- n;
          d_pos[i] <- d;
          i <- i + 1;
        } else {
          n_neg[j] <- n;
          d_neg[j] <- d;
          j <- j + 1;
        }
      }
    }
  }
}
```

変数`N_pos`と`N_neg`には、観測値`y`の真(1)の数と偽(1)の数とが入ります。そのループではそれから、真と偽の値についてのインデックスの列で4つの配列を埋めます。

パラメータは以下のように宣言されます。

```
parameters {
  matrix[D,K] beta;
  cholesky_factor_corr[D] L_Omega;
  vector<lower=0>[N_pos] z_pos;
  vector<upper=0>[N_neg] z_neg;
}
```

ここには、回帰係数`beta`と、相関行列のコレスキー因子`L_Omega`が含まれます。共分散行列は単位スケール（すなわち相関行列です。上を参照）なので、今回はスケーリングはありません。

パラメータ宣言の重要なところは、潜在実数変数`z`が真のみの成分と、偽のみの成分とに分解されていることです。そのサイズは、便利なことに`transformed data`ブロックで計算されています。`transformed data`ブロックの実際の仕事は、`transformed parameters`ブロックで`z`を復元できるようにすることでした。

```
transformed parameters {
  vector[D] z[N];
  for (n in 1:N_pos)
    z[n_pos[n], d_pos[n]] <- z_pos[n];
  for (n in 1:N_neg)
    z[n_neg[n], d_neg[n]] <- z_neg[n];
}
```

ここまで来るとモデルは単純で、見かけ上無関係な回帰とかなりよく似ています。

```
model {
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(beta) ~ normal(0, 5);
  {
    vector[D] beta_x[N];
    for (n in 1:N)
      beta_x[n] <- beta * x[n];
    z ~ multi_normal_cholesky(beta_x, L_Omega);
  }
}
```

モデルがこのように単純になったのは、`z`についてのAlbertとChib式の制約のおかげです。

最後に、相関行列自体が必要なら、`generated quantities`ブロックで戻すことができます。

```
generated quantities {
  corr_matrix[D] Omega;
  Omega <- multiply_lower_tri_self_transpose(L_Omega);
}
```

もちろん、前の節の見かけ上無関係な回帰でも同じことができるでしょう。
